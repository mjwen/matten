seed_everything: 35
log_level: debug

data:
  class_path: eigenn.dataset.LSDI.SiNMRDataMoldule
  init_args:
    root: ../eigenn/dataset/LSDI_NMR
    trainset_filename: LSDI_NMR_tensor.json
    valset_filename: LSDI_NMR_tensor.json
    testset_filename: LSDI_NMR_tensor.json
    r_cut: 5.0
    loader_kwargs:
      batch_size: 10

model:
  backbone_hparams:
    #
    # input embedding
    #

    # atom species embedding
    species_embedding_dim: 32
    # species_embedding_irreps_out: 16x0e # not needed when using SpeciesEmbedding

    # spherical harmonics embedding of edge direction
    irreps_edge_sh: 0e + 1o + 2e

    # radial edge distance embedding
    num_radial_basis: 8
    radial_basis_r_cut: 5.0 # should be the same as r_cut in dataset

    #
    # message passing conv layers
    #
    num_layers: 3

    # radial network
    invariant_layers: 2 # number of radial layers, we found it important to keep this small, 1 or 2
    invariant_neurons: 64 # number of hidden neurons in radial function, smaller is faster
    avg_num_neighbors: null # number of neighbors to divide by, None => no normalization.
    use_sc: true # use self connection? `true` seems always improve performance

    conv_layer_irreps: 32x0o + 32x0e + 16x1o + 16x1e + 8x2o + 8x2e
    nonlinearity_type: gate
    resnet: true

    #
    # output
    #
    conv_to_output_hidden_irreps_out: 16x0e + 8x1e + 4x2e # should contain 0e and 2e for symmetric 2nd-order tensor
    output_formula: ij=ji # symmetric 2nd-order tensor (see e3nn.io.CartesianTensor)

  task_hparams:
    task_name: tensor_output # name of y in dataset to predict

trainer:
  max_epochs: 2 # number of maximum training epochs
  num_nodes: 1
  gpus: null
  terminate_on_nan: true
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val/score
        mode: min
        save_top_k: 3
        save_last: true
        verbose: False
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/score
        mode: min
        patience: 100
        min_delta: 0
        verbose: true
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      save_dir: wandb_logs # should be provided to make cli save config work
      project: tmp-eigenn

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 0.001

lr_scheduler:
  class_path: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
  init_args:
    warmup_epochs: 10
    max_epochs: 20 # this should be set to trainer.max_epochs
    eta_min: 0.0
