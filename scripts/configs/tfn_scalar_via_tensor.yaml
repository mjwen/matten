seed_everything: 35
log_level: info

data:
  class_path: matten.dataset.structure_scalar_tensor.TensorDataModule
  init_args:
    root: /Users/mjwen.admin/Packages/matten_analysis/matten_analysis/dataset/elastic_tensor/20220523
    r_cut: 5.0
    trainset_filename: crystal_elasticity_n20.json
    valset_filename: crystal_elasticity_n20.json
    testset_filename: crystal_elasticity_n20.json
    #
    tensor_target_name: null
    tensor_target_format: irreps
    tensor_target_formula: ijkl=jikl=klij
    normalize_tensor_target: false
    #
    scalar_target_names: [k_voigt, k_reuss] # should be in `tasks` below
    #
    global_featurizer: null
    #
    reuse: false
    compute_dataset_statistics: true # need be false when using cartesian tensor
    loader_kwargs:
      batch_size: 10
      shuffle: true

model:
  backbone_hparams:
    #
    # input embedding
    #

    # atom species embedding
    species_embedding_dim: 32
    # species_embedding_irreps_out: 16x0e # not needed when using SpeciesEmbedding

    # spherical harmonics embedding of edge direction
    irreps_edge_sh: 0e + 1o + 2e + 3o + 4e

    # radial edge distance embedding
    num_radial_basis: 10
    radial_basis_start: 0.
    radial_basis_end: 5.

    #
    # message passing conv layers
    #
    num_layers: 3

    # radial network
    invariant_layers: 2 # number of radial layers, we found it important to keep this small, 1 or 2
    invariant_neurons: 32 # number of hidden neurons in radial function, smaller is faster
    average_num_neighbors: null # average number of neighbors, used for normalization . Options: 1. float or int provided here. 2. `auto` to determine it automatically, by setting it to average of dataset 3. `null` to not use it, and then individual normalization will be applied to each node
    conv_layer_irreps: 32x0o+32x0e + 16x1o+16x1e + 8x2o+8x2e + 4x3o+4x3e + 4x4o+4x4e
    nonlinearity_type: gate
    normalization: none # {`none`, `batch`, `instance`}
    resnet: true

    #
    # output
    # 2x2e and 4e need to be fixed, and only multiplicity of 0e can change: where we
    # combine global features
    conv_to_output_hidden_irreps_out: 16x0e + 2x2e + 4e

    # output_format and output_formula should be used together.
    # - output_format (can be `irreps` or `cartesian`) determines what the loss
    #   function (if any) will be on (either on the irreps space or the cartesian
    #   space). It should always be the same as `output_format` in the dada
    #   section above.
    # - output_formula gives what the cartesian formula is when e.g. ij=ji gives a
    #   symmetric second order tensor. When output_format=`irreps`, this formula will
    #   be used to convert the output to its corresponding irreps (e.g. 0e+2e for ij=ji)
    #   to build the loss (if any). It should always be the same as `output_formula`
    #   in the dada section above.
    output_format: irreps
    output_formula: ijkl=jikl=klij

    # pooling node feats to graph feats
    reduce: mean

  # tasks define the loss, metric...
  tasks:
    - class_path: matten.model_factory.task.ScalarRegressionTask
      init_args:
        name: k_voigt
        loss_weight: 1.
    - class_path: matten.model_factory.task.ScalarRegressionTask
      init_args:
        name: k_reuss
        loss_weight: 1.

trainer:
  max_epochs: 5 # number of maximum training epochs
  num_nodes: 1
  gpus: null
  # detect_anomaly: true # DEBUG
  # track_grad_norm: 2 # DEBUG
  # gradient_clip_val: 0.5
  # gradient_clip_algorithm: norm
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: val/score
        mode: min
        save_top_k: 3
        save_last: true
        verbose: false
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/score
        mode: min
        patience: 200
        min_delta: 0
        verbose: true
    - class_path: pytorch_lightning.callbacks.ModelSummary
      init_args:
        max_depth: -1
  #    - class_path: pytorch_lightning.callbacks.StochasticWeightAveraging
  #      init_args:
  #        swa_epoch_start: 0.8
  #        swa_lrs: null # use default learning rate for swa
  logger:
    class_path: pytorch_lightning.loggers.wandb.WandbLogger
    init_args:
      save_dir: wandb_logs # should be provided to make cli save config work
      project: tmp-matten

optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 0.001
    weight_decay: 0.00001

lr_scheduler:
  class_path: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
  init_args:
    warmup_epochs: 10
    max_epochs: 20 # this should be set to trainer.max_epochs
    eta_min: 0.0
